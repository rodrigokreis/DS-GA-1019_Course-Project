{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BASELINE VERSION\n",
    "def sigmoid(z):\n",
    "    sigmoid_vals = []\n",
    "    for elem in z:\n",
    "        sigmoid_vals.append(1.0 / (1.0 + math.exp(-elem)))\n",
    "\n",
    "    sigmoid_array = np.array(sigmoid_vals)\n",
    "    sigmoid_array = sigmoid_array.reshape((len(sigmoid_vals), 1))\n",
    "    return sigmoid_array\n",
    "\n",
    "\n",
    "\n",
    "def loss(w, X, y):\n",
    "    l = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        margin = 0\n",
    "        \n",
    "        for j in range(len(w)):\n",
    "            margin += X[i][j] * w[j][0]\n",
    "        \n",
    "        l_if_pos = -math.log(1 + math.exp(-margin)) * y[i][0]\n",
    "        l_if_neg = -math.log(1 + math.exp(margin)) * (1 - y[i][0])\n",
    "        \n",
    "        l += -(l_if_pos + l_if_neg)\n",
    "    return l\n",
    "\n",
    "\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    m = len(X)\n",
    "    n = len(X[0])\n",
    "\n",
    "    diff_all_m = (y_hat - y)/m\n",
    "\n",
    "    X_t = X.T\n",
    "\n",
    "    dw_list = []\n",
    "    for j in range(n):\n",
    "        dw_j = np.dot(X_t[j], diff_all_m)\n",
    "        dw_list.append(dw_j)\n",
    "    dw = np.array(dw_list)\n",
    "    dw = dw.reshape((len(dw_list), 1))\n",
    "\n",
    "    db_array = sum([y_hat[i] - y[i] for i in range(m)]) / m\n",
    "    db = db_array.item()\n",
    "    return dw, db  \n",
    "\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    m, n = len(X), len(X[0])\n",
    "    \n",
    "    # Compute column-wise means and standard deviations\n",
    "    means = [0] * n\n",
    "    stds = [0] * n\n",
    "    for j in range(n):\n",
    "        col_sum = 0\n",
    "        for i in range(m):\n",
    "            col_sum += X[i][j]\n",
    "        means[j] = col_sum / m\n",
    "        \n",
    "        col_var = 0\n",
    "        for i in range(m):\n",
    "            col_var += (X[i][j] - means[j]) ** 2\n",
    "        stds[j] = (col_var / m) ** 0.5\n",
    "    \n",
    "    # Normalize X based on means and standard deviations\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            X[i][j] = (X[i][j] - means[j]) / stds[j]\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1))\n",
    "    b = 0\n",
    "    \n",
    "    # Normalize inputs\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # Store losses\n",
    "    losses = []\n",
    "    \n",
    "    # Train\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "    # for epoch in range(epochs):\n",
    "        for i in range((m-1)//bs + 1):\n",
    "            \n",
    "            # Defining batches for SGD (this can be changed)\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Predict\n",
    "            y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            # Update params\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calc loss\n",
    "        l = loss(w, x, y)\n",
    "        losses.append(l)\n",
    "    return w, b, losses#, dw, db, xb, yb, y_hat, x, y\n",
    "\n",
    "\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    # X --> Input.\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # Calculating presictions/y_hat.\n",
    "    preds = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    # if y_hat >= 0.5 --> round up to 1\n",
    "    # if y_hat < 0.5 --> round up to 1\n",
    "    pred_class = []\n",
    "    for i in preds:\n",
    "        if i > 0.5:\n",
    "            pred_class.append(1)\n",
    "        else:\n",
    "            pred_class.append(0)\n",
    "    return np.array(pred_class)\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    y = np.array(y).ravel()\n",
    "    count = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == y_hat[i]:\n",
    "            count += 1\n",
    "\n",
    "    accuracy = count / len(y)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def compare(X, y):\n",
    "    # Training \n",
    "    start1 = time.time()\n",
    "    w, b, l = train(X, y, bs=100, epochs=1000, lr=0.001)\n",
    "    pred = predict(X, w, b)\n",
    "    acc = accuracy(y, pred)\n",
    "    end1 = time.time()\n",
    "    print(f'Time to run our logistic regression: {end1 - start1} s')\n",
    "    print(f'Accuracy of our logistic regression: {acc}')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the parquet file took 8.5037 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_raw = pd.read_parquet('./data/train_data.parquet')\n",
    "end = time.time() \n",
    "print(f\"Reading the parquet file took {end - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_getSampledata(train_raw):\n",
    "    raw_sample = train_raw.iloc[:100_000] #Change sample size here\n",
    "    raw_sample = raw_sample.drop('B_31', axis='columns')\n",
    "    sample = raw_sample.select_dtypes(include=['float32', 'int64'], exclude=['object', 'category']).fillna(0)\n",
    "    categorical_features = ['target']\n",
    "    sample[categorical_features] = sample[categorical_features].astype(\"float32\")\n",
    "    X_train = sample.iloc[:,:-1].values\n",
    "    y_train = sample[['target']].values\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_and_getSampledata(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:06<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run our logistic regression: 126.9586329460144 s\n",
      "Accuracy of our logistic regression: 0.87453\n"
     ]
    }
   ],
   "source": [
    "w2 = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# w2 = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# w2 = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f train w, b, losses = train(X_train, y_train, bs=100, epochs=1000, lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy version: 10_000 rows\n",
    "\n",
    "NUMPY version with 10_000 rows\n",
    "\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 8.83284 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_7388/2755537311.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       5000.0   5000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1      19000.0  19000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1       1000.0   1000.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1   32351000.0 32351000.0      0.4      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1       3000.0   3000.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15                                               # for epoch in tqdm(range(epochs)):\n",
    "    16      1000     672000.0    672.0      0.0      for epoch in range(epochs):\n",
    "    17    100000   39397000.0    394.0      0.4          for i in range((m-1)//bs + 1):\n",
    "    18                                                       \n",
    "    19                                                       # Defining batches for SGD (this can be changed)\n",
    "    20    100000   35944000.0    359.4      0.4              start_i = i*bs\n",
    "    21    100000   36371000.0    363.7      0.4              end_i = start_i + bs\n",
    "    22    100000   80850000.0    808.5      0.9              xb = x[start_i:end_i]\n",
    "    23    100000   51996000.0    520.0      0.6              yb = y[start_i:end_i]\n",
    "    24                                                       \n",
    "    25                                                       # Predict\n",
    "    26    100000 3408959000.0  34089.6     38.6              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    27                                                       \n",
    "    28                                                       # Calculate gradients\n",
    "    29    100000 3699699000.0  36997.0     41.9              dw, db = gradients(xb, yb, y_hat)\n",
    "    30                                                       \n",
    "    31                                                       # Update params\n",
    "    32    100000  279678000.0   2796.8      3.2              w -= lr*dw\n",
    "    33    100000   53445000.0    534.5      0.6              b -= lr*db\n",
    "    34                                                   \n",
    "    35                                                   # Calc loss\n",
    "    36      1000 1111383000.0 1111383.0     12.6          l = loss(w, x, y)\n",
    "    37      1000    2070000.0   2070.0      0.0          losses.append(l)\n",
    "    38                                                   \n",
    "    39         1       1000.0   1000.0      0.0      return w, b, losses, dw, db, xb, yb, y_hat, x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slowest Version: 10_000 rows\n",
    "\n",
    "Slower Loss Function with 10_000 rows\n",
    "\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 1245.67 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_6936/2755537311.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       3000.0   3000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1      12000.0  12000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1          0.0      0.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1 2920546000.0 2920546000.0      0.2      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1          0.0      0.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15                                               # for epoch in tqdm(range(epochs)):\n",
    "    16      1000     604000.0    604.0      0.0      for epoch in range(epochs):\n",
    "    17    100000   46172000.0    461.7      0.0          for i in range((m-1)//bs + 1):\n",
    "    18                                                       \n",
    "    19                                                       # Defining batches for SGD (this can be changed)\n",
    "    20    100000   49060000.0    490.6      0.0              start_i = i*bs\n",
    "    21    100000   48828000.0    488.3      0.0              end_i = start_i + bs\n",
    "    22    100000  104667000.0   1046.7      0.0              xb = x[start_i:end_i]\n",
    "    23    100000   59746000.0    597.5      0.0              yb = y[start_i:end_i]\n",
    "    24                                                       \n",
    "    25                                                       # Predict\n",
    "    26    100000 16590260000.0 165902.6      1.3              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    27                                                       \n",
    "    28                                                       # Calculate gradients\n",
    "    29    100000 68910900000.0 689109.0      5.5              dw, db = gradients(xb, yb, y_hat)\n",
    "    30                                                       \n",
    "    31                                                       # Update params\n",
    "    32    100000  477482000.0   4774.8      0.0              w -= lr*dw\n",
    "    33    100000   55244000.0    552.4      0.0              b -= lr*db\n",
    "    34                                                   \n",
    "    35                                                   # Calc loss\n",
    "    36      1000 1156398994000.0 1156398994.0     92.8          l = loss(w, x, y)\n",
    "    37      1000    2751000.0   2751.0      0.0          losses.append(l)\n",
    "    38                                                   \n",
    "    39         1       1000.0   1000.0      0.0      return w, b, losses, dw, db, xb, yb, y_hat, x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slower gradients function over 10_000 rows\n",
    "\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 97.2686 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_6380/2755537311.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       4000.0   4000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1      15000.0  15000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1          0.0      0.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1 3182344000.0 3182344000.0      3.3      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1       1000.0   1000.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15                                               # for epoch in tqdm(range(epochs)):\n",
    "    16      1000     762000.0    762.0      0.0      for epoch in range(epochs):\n",
    "    17    100000   49147000.0    491.5      0.1          for i in range((m-1)//bs + 1):\n",
    "    18                                                       \n",
    "    19                                                       # Defining batches for SGD (this can be changed)\n",
    "    20    100000   51853000.0    518.5      0.1              start_i = i*bs\n",
    "    21    100000   56232000.0    562.3      0.1              end_i = start_i + bs\n",
    "    22    100000  120018000.0   1200.2      0.1              xb = x[start_i:end_i]\n",
    "    23    100000   67372000.0    673.7      0.1              yb = y[start_i:end_i]\n",
    "    24                                                       \n",
    "    25                                                       # Predict\n",
    "    26    100000 16504809000.0 165048.1     17.0              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    27                                                       \n",
    "    28                                                       # Calculate gradients\n",
    "    29    100000 75450204000.0 754502.0     77.6              dw, db = gradients(xb, yb, y_hat)\n",
    "    30                                                       \n",
    "    31                                                       # Update params\n",
    "    32    100000  515040000.0   5150.4      0.5              w -= lr*dw\n",
    "    33    100000   67845000.0    678.5      0.1              b -= lr*db\n",
    "    34                                                   \n",
    "    35                                                   # Calc loss\n",
    "    36      1000 1201491000.0 1201491.0      1.2          l = loss(w, x, y)\n",
    "    37      1000    1483000.0   1483.0      0.0          losses.append(l)\n",
    "    38                                                   \n",
    "    39         1       1000.0   1000.0      0.0      return w, b, losses, dw, db, xb, yb, y_hat, x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTREEEEEEEMLY SLOW gradients function on 200 rows only\n",
    "\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 20.9818 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_5342/2755537311.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       2000.0   2000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1       6000.0   6000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1          0.0      0.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1   12975000.0 12975000.0      0.1      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1       1000.0   1000.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15                                               # for epoch in tqdm(range(epochs)):\n",
    "    16      1000     600000.0    600.0      0.0      for epoch in range(epochs):\n",
    "    17      2000    2656000.0   1328.0      0.0          for i in range((m-1)//bs + 1):\n",
    "    18                                                       \n",
    "    19                                                       # Defining batches for SGD (this can be changed)\n",
    "    20      2000    1196000.0    598.0      0.0              start_i = i*bs\n",
    "    21      2000     903000.0    451.5      0.0              end_i = start_i + bs\n",
    "    22      2000    3723000.0   1861.5      0.0              xb = x[start_i:end_i]\n",
    "    23      2000    1261000.0    630.5      0.0              yb = y[start_i:end_i]\n",
    "    24                                                       \n",
    "    25                                                       # Predict\n",
    "    26      2000  279952000.0 139976.0      1.3              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    27                                                       \n",
    "    28                                                       # Calculate gradients\n",
    "    29      2000 20562835000.0 10281417.5     98.0              dw, db = gradients(xb, yb, y_hat)\n",
    "    30                                                       \n",
    "    31                                                       # Update params\n",
    "    32      2000   12389000.0   6194.5      0.1              w -= lr*dw\n",
    "    33      2000    2388000.0   1194.0      0.0              b -= lr*db\n",
    "    34                                                   \n",
    "    35                                                   # Calc loss\n",
    "    36      1000   99422000.0  99422.0      0.5          l = loss(w, x, y)\n",
    "    37      1000    1456000.0   1456.0      0.0          losses.append(l)\n",
    "    38                                                   \n",
    "    39         1       1000.0   1000.0      0.0      return w, b, losses, dw, db, xb, yb, y_hat, x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ITERATIVE SIGMOID function\n",
    "\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 22.8768 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_3505/2337235970.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       3000.0   3000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1      12000.0  12000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1       1000.0   1000.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1 3408265000.0 3408265000.0     14.9      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1       1000.0   1000.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15      1000     876000.0    876.0      0.0      for epoch in range(epochs):\n",
    "    16    100000   42802000.0    428.0      0.2          for i in range((m-1)//bs + 1):\n",
    "    17                                                       \n",
    "    18                                                       # Defining batches for SGD (this can be changed)\n",
    "    19    100000   37798000.0    378.0      0.2              start_i = i*bs\n",
    "    20    100000   40305000.0    403.1      0.2              end_i = start_i + bs\n",
    "    21    100000   92731000.0    927.3      0.4              xb = x[start_i:end_i]\n",
    "    22    100000   51316000.0    513.2      0.2              yb = y[start_i:end_i]\n",
    "    23                                                       \n",
    "    24                                                       # Predict\n",
    "    25    100000 13611616000.0 136116.2     59.5              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    26                                                       \n",
    "    27                                                       # Calculate gradients\n",
    "    28    100000 4155308000.0  41553.1     18.2              dw, db = gradients(xb, yb, y_hat)\n",
    "    29                                                       \n",
    "    30                                                       # Update params\n",
    "    31    100000  290947000.0   2909.5      1.3              w -= lr*dw\n",
    "    32    100000   60531000.0    605.3      0.3              b -= lr*db\n",
    "    33                                                   \n",
    "    34                                                   # Calc loss\n",
    "    35      1000 1082679000.0 1082679.0      4.7          l = loss(w, x, y)\n",
    "    36      1000    1577000.0   1577.0      0.0          losses.append(l)\n",
    "    37                                                   \n",
    "    38         1          0.0      0.0      0.0      return w, b, losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 12.0944 s\n",
    "File: /var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2969/2337235970.py\n",
    "Function: train at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def train(X, y, bs, epochs, lr):\n",
    "     2         1       3000.0   3000.0      0.0      m, n = X.shape\n",
    "     3                                               \n",
    "     4                                               # Initializing weights and bias to zeros.\n",
    "     5         1      32000.0  32000.0      0.0      w = np.zeros((n,1))\n",
    "     6         1       1000.0   1000.0      0.0      b = 0\n",
    "     7                                               \n",
    "     8                                               # Normalize inputs\n",
    "     9         1 3052914000.0 3052914000.0     25.2      x = normalize(X)\n",
    "    10                                               \n",
    "    11                                               # Store losses\n",
    "    12         1          0.0      0.0      0.0      losses = []\n",
    "    13                                               \n",
    "    14                                               # Train\n",
    "    15      1000     826000.0    826.0      0.0      for epoch in range(epochs):\n",
    "    16    100000   42030000.0    420.3      0.3          for i in range((m-1)//bs + 1):\n",
    "    17                                                       \n",
    "    18                                                       # Defining batches for SGD (this can be changed)\n",
    "    19    100000   36225000.0    362.2      0.3              start_i = i*bs\n",
    "    20    100000   36684000.0    366.8      0.3              end_i = start_i + bs\n",
    "    21    100000   83706000.0    837.1      0.7              xb = x[start_i:end_i]\n",
    "    22    100000   53319000.0    533.2      0.4              yb = y[start_i:end_i]\n",
    "    23                                                       \n",
    "    24                                                       # Predict\n",
    "    25    100000 3582322000.0  35823.2     29.6              y_hat = sigmoid(np.dot(xb, w) + b)\n",
    "    26                                                       \n",
    "    27                                                       # Calculate gradients\n",
    "    28    100000 3859054000.0  38590.5     31.9              dw, db = gradients(xb, yb, y_hat)\n",
    "    29                                                       \n",
    "    30                                                       # Update params\n",
    "    31    100000  287882000.0   2878.8      2.4              w -= lr*dw\n",
    "    32    100000   52495000.0    525.0      0.4              b -= lr*db\n",
    "    33                                                   \n",
    "    34                                                   # Calc loss\n",
    "    35      1000 1005112000.0 1005112.0      8.3          l = loss(w, x, y)\n",
    "    36      1000    1817000.0   1817.0      0.0          losses.append(l)\n",
    "    37                                                   \n",
    "    38         1       1000.0   1000.0      0.0      return w, b, losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
