{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, float64, float32, int32, prange, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:79: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  y_hat = 1.0 / (1 + np.exp(-(np.dot(xb, w) + b)))\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:79: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\n",
      "  y_hat = 1.0 / (1 + np.exp(-(np.dot(xb, w) + b)))\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:89: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  l = loss(w, x, y)\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:89: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  l = loss(w, x, y)\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:79: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  y_hat = 1.0 / (1 + np.exp(-(np.dot(xb, w) + b)))\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:79: NumbaPerformanceWarning: \u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\n",
      "  y_hat = 1.0 / (1 + np.exp(-(np.dot(xb, w) + b)))\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:123: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  pred = predict(X, w, b)\n",
      "/var/folders/9c/9y0zmgk55297pv9nj1zpn02c0000gn/T/ipykernel_2770/660448392.py:123: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1mnp.dot() is faster on contiguous arrays, called on (array(float32, 2d, A), array(float32, 2d, C))\u001b[0m\u001b[0m\u001b[0m\n",
      "  pred = predict(X, w, b)\n"
     ]
    }
   ],
   "source": [
    "@njit(fastmath=True, nogil=True)\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "@njit(nogil=True)\n",
    "def loss(w, X, y):\n",
    "    margin = np.dot(X, w)\n",
    "    l_if_pos = -np.logaddexp(0, -margin) * y\n",
    "    l_if_neg = -np.logaddexp(0, margin) * (1 - y)\n",
    "    \n",
    "    l = -(l_if_pos + l_if_neg)\n",
    "    sum_l = np.sum(l)\n",
    "    return sum_l\n",
    "\n",
    "\n",
    "@njit(\"(float32[:, :],float32[:, :],float64[:, :])\", fastmath=True, nogil=True)\n",
    "def gradients(X, y, y_hat):\n",
    "    m = X.shape[0]\n",
    "    diff = np.subtract(y_hat, y)\n",
    "    scale_factor = np.divide(1, m)\n",
    "    \n",
    "    # Gradient of loss w.r.t weights\n",
    "    dot = np.dot(np.transpose(X).astype(np.float64), diff)\n",
    "    dw = np.multiply(scale_factor,dot)\n",
    "        \n",
    "    # Gradient of loss w.r.t bias\n",
    "    db = np.multiply(scale_factor, np.sum(diff))\n",
    "    return dw, db\n",
    "\n",
    "\n",
    "@njit(\"float32[:, :](float32[:, :])\", nogil=True, parallel=True)\n",
    "def normalize(X):\n",
    "    n, m = X.shape\n",
    "    means = np.zeros(m)\n",
    "    stds = np.zeros(m)\n",
    "    \n",
    "    # Compute column-wise means and standard deviations\n",
    "    for j in prange(m):\n",
    "        col = X[:, j]\n",
    "        means[j] = np.mean(col)\n",
    "        stds[j] = np.std(col)\n",
    "    \n",
    "    # Normalize X based on means and standard deviations\n",
    "    for i in prange(n):\n",
    "        for j in prange(m):\n",
    "            X[i, j] = (X[i, j] - means[j]) / stds[j]\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "@njit(\"(float32[:, :],float32[:, :],int32,int32,float32)\", nogil=True, parallel=True, fastmath=True)\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    el = len(range(epochs))\n",
    "    \n",
    "    # Initializing weights and bias to zeros.\n",
    "    w = np.zeros((n,1), dtype=np.float32)\n",
    "    b = 0.0\n",
    "    \n",
    "    # Normalize inputs\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # Store losses\n",
    "    np_losses = np.empty(el, dtype=np.float32)\n",
    "    \n",
    "    # Train\n",
    "    for epoch in prange(epochs):\n",
    "        for i in prange((m-1)//bs + 1):\n",
    "            \n",
    "            # Defining batches for SGD\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            # Predict\n",
    "            y_hat = 1.0 / (1 + np.exp(-(np.dot(xb, w) + b)))\n",
    "            \n",
    "            # Calculate gradients\n",
    "            dw, db = gradients(xb, yb, y_hat)\n",
    "            \n",
    "            # Update params\n",
    "            w -= lr*dw\n",
    "            b -= lr*db\n",
    "        \n",
    "        # Calc loss\n",
    "        l = loss(w, x, y)\n",
    "        np_losses[epoch] = l\n",
    "        \n",
    "    return w, b, np_losses\n",
    "\n",
    "@njit\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    # Normalizing the inputs.\n",
    "    x = normalize(X)\n",
    "    \n",
    "    # Calculating presictions/y_hat.\n",
    "    preds = sigmoid(np.dot(x, w) + b)\n",
    "    \n",
    "    # Converting predicted probabilities to binary classes.\n",
    "    pred_class = np.where(preds >= 0.5, 1, 0)\n",
    "    \n",
    "    return pred_class\n",
    "\n",
    "@njit\n",
    "def accuracy(y, pred):\n",
    "    y_arr = np.asarray(y, dtype=np.float32)\n",
    "    y_rav = y_arr.ravel()\n",
    "\n",
    "    # pred = np.array(pred).ravel()\n",
    "    pred_arr = np.asarray(pred, dtype=np.float32)\n",
    "    pred_rav = pred_arr.ravel()\n",
    "    return  np.sum(y_rav == pred_rav) / len(y_rav)\n",
    "\n",
    "@njit(\"(float32[:, :],float32[:, :])\", fastmath=True, nogil=True)\n",
    "def compare(X, y):\n",
    "    random.seed(1)\n",
    "    # Training \n",
    "    w, b, l = train(X, y, bs=100, epochs=1000, lr=0.001)\n",
    "    pred = predict(X, w, b)\n",
    "    acc = accuracy(y, pred)\n",
    "\n",
    "    return w, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the parquet file took 4.9624 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_raw = pd.read_parquet('./data/train_data.parquet')\n",
    "end = time.time() \n",
    "print(f\"Reading the parquet file took {end - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_getSampledata(train_raw):\n",
    "    raw_sample = train_raw.iloc[:100_000] #Change sample size here\n",
    "    raw_sample = raw_sample.drop('B_31', axis='columns')\n",
    "    sample = raw_sample.select_dtypes(include=['float32', 'int64'], exclude=['object', 'category']).fillna(0)\n",
    "    categorical_features = ['target']\n",
    "    sample[categorical_features] = sample[categorical_features].astype(\"float32\")\n",
    "    X_train = sample.iloc[:,:-1].values\n",
    "    y_train = sample[['target']].values\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = clean_and_getSampledata(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2, acc = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# w2, acc = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# w2, acc = compare(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Accuracy of our logistic regression: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
